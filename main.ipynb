{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# LegalAI RAG Pipeline\n",
                "\n",
                "This notebook implements the RAG pipeline using:\n",
                "- **Pinecone Vector DB** (Scalable storage)\n",
                "- **NER Redaction** (Privacy protection)\n",
                "- **Legal-Specific Chunking** (Regex-based)\n",
                "- **GPU Acceleration** (Google Colab optimized)\n",
                "- **Comprehensive Metrics** (M1-M24 including BERTScore, FCD, Bias)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- 1. SETUP & DEPENDENCIES ---\n",
                "import sys\n",
                "import os\n",
                "\n",
                "IN_COLAB = 'google.colab' in sys.modules\n",
                "\n",
                "if IN_COLAB:\n",
                "    print(\"Running in Google Colab. Installing dependencies...\")\n",
                "    !pip install -q PyMuPDF==1.23.26 sentence-transformers pinecone psutil nltk rouge-score bert-score tiktoken transformers torch scikit-learn\n",
                "    !pip install -q -U google-generativeai\n",
                "else:\n",
                "    print(\"Running Locally. Ensure 'requirements.txt' are installed.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- 2. IMPORTS & CONFIG ---\n",
                "import re\n",
                "import time\n",
                "import psutil\n",
                "import warnings\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from tqdm.notebook import tqdm\n",
                "from pathlib import Path\n",
                "\n",
                "import torch\n",
                "from transformers import logging as hf_logging\n",
                "hf_logging.set_verbosity_error()\n",
                "warnings.filterwarnings(\"ignore\", message=\"Some weights of the model\")\n",
                "\n",
                "# NLTK\n",
                "import nltk\n",
                "for res in ['wordnet', 'omw-1.4', 'punkt']:\n",
                "    try: nltk.data.find(f'corpora/{res}')\n",
                "    except LookupError: nltk.download(res, quiet=True)\n",
                "\n",
                "from sentence_transformers import SentenceTransformer\n",
                "from rouge_score import rouge_scorer\n",
                "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
                "from nltk.translate.meteor_score import meteor_score\n",
                "from bert_score import score as bert_score\n",
                "from sklearn.metrics.pairwise import cosine_similarity\n",
                "import tiktoken\n",
                "\n",
                "# --- API KEY INPUTS (SECURE) ---\n",
                "import google.generativeai as genai\n",
                "if IN_COLAB:\n",
                "    from google.colab import userdata\n",
                "else:\n",
                "    userdata = None\n",
                "\n",
                "def get_key(name):\n",
                "    try:\n",
                "        return userdata.get(name)\n",
                "    except:\n",
                "        return os.getenv(name)\n",
                "\n",
                "print(\"\\nüîê API CONFIGuration\")\n",
                "GOOGLE_API_KEY = get_key(\"GOOGLE_API_KEY\")\n",
                "if not GOOGLE_API_KEY:\n",
                "    GOOGLE_API_KEY = input(\"Enter GOOGLE_API_KEY: \").strip()\n",
                "genai.configure(api_key=GOOGLE_API_KEY)\n",
                "\n",
                "PINECONE_API_KEY = get_key(\"PINECONE_API_KEY\")\n",
                "if not PINECONE_API_KEY:\n",
                "    PINECONE_API_KEY = input(\"Enter PINECONE_API_KEY: \").strip()\n",
                "\n",
                "print(\"‚úÖ Keys configured\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- 3. PINECONE SETUP ---\n",
                "try:\n",
                "    from pinecone import Pinecone, ServerlessSpec\n",
                "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
                "    PINECONE_VERSION = \"v3\"\n",
                "except ImportError:\n",
                "    import pinecone\n",
                "    pinecone.init(api_key=PINECONE_API_KEY, environment=\"us-east-1\")\n",
                "    PINECONE_VERSION = \"v2\"\n",
                "    \n",
                "INDEX_NAME = \"mpnet-index-colab\"\n",
                "\n",
                "# Auto-create index\n",
                "existing_indexes = [i.name for i in pc.list_indexes()] if PINECONE_VERSION == \"v3\" else pinecone.list_indexes()\n",
                "\n",
                "if INDEX_NAME not in existing_indexes:\n",
                "    print(f\"Creating index '{INDEX_NAME}'...\")\n",
                "    if PINECONE_VERSION == \"v3\":\n",
                "        pc.create_index(\n",
                "            name=INDEX_NAME,\n",
                "            dimension=768,\n",
                "            metric=\"cosine\",\n",
                "            spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
                "        )\n",
                "    else:\n",
                "        pinecone.create_index(name=INDEX_NAME, dimension=768, metric=\"cosine\")\n",
                "\n",
                "if PINECONE_VERSION == \"v3\":\n",
                "    index = pc.Index(INDEX_NAME)\n",
                "else:\n",
                "    index = pinecone.Index(INDEX_NAME)\n",
                "\n",
                "print(f\"‚úÖ Connected to Pinecone Index: {INDEX_NAME}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- 4. NER & PDF UTILS ---\n",
                "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
                "\n",
                "# Optional Redaction\n",
                "USE_NER_REDACTION = input(\"Use NER Redaction to hide names? (y/n) [n]: \").lower() == 'y'\n",
                "\n",
                "ner_pipeline = None\n",
                "if USE_NER_REDACTION:\n",
                "    print(\"Loading NER model...\")\n",
                "    ner_model_name = \"dslim/bert-base-NER\"\n",
                "    ner_tokenizer = AutoTokenizer.from_pretrained(ner_model_name)\n",
                "    ner_model = AutoModelForTokenClassification.from_pretrained(ner_model_name)\n",
                "    ner_pipeline = pipeline(\"ner\", model=ner_model, tokenizer=ner_tokenizer, aggregation_strategy=\"simple\")\n",
                "    print(\"‚úÖ NER Ready\")\n",
                "\n",
                "try:\n",
                "    import fitz\n",
                "    PDF_LIBRARY = \"pymupdf\"\n",
                "except ImportError:\n",
                "    from pypdf import PdfReader\n",
                "    PDF_LIBRARY = \"pypdf\"\n",
                "\n",
                "def extract_chunks(pdf_path, chunk_size=200):\n",
                "    chunks = []\n",
                "    # Advanced Regex for legal citations\n",
                "    citation_pattern = re.compile(r\"(Section\\s\\d+[A-Za-z]|Sec\\.\\s\\d+[A-Za-z]|\\d+\\s?Cr\\.?\\s?\\d+)\", re.IGNORECASE)\n",
                "\n",
                "    try:\n",
                "        if PDF_LIBRARY == \"pymupdf\":\n",
                "            doc = fitz.open(pdf_path)\n",
                "            text = \"\\n\".join([page.get_text(\"text\") for page in doc])\n",
                "            doc.close()\n",
                "        else:\n",
                "            reader = PdfReader(pdf_path)\n",
                "            if reader.is_encrypted:\n",
                "                try: reader.decrypt('')\n",
                "                except: pass\n",
                "            text = \"\\n\".join([page.extract_text() or \"\" for page in reader.pages])\n",
                "\n",
                "        text = re.sub(r\"[*_]\", \"\", text)\n",
                "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
                "        sentences = re.split(r'(?<=[.?!]) +|\\n+', text)\n",
                "\n",
                "        buf = \"\"\n",
                "        for sent in sentences:\n",
                "            parts = citation_pattern.split(sent)\n",
                "            for part in parts:\n",
                "                seg = part.strip()\n",
                "                if not seg: continue\n",
                "                if len(buf.split()) + len(seg.split()) < chunk_size:\n",
                "                    buf += \" \" + seg\n",
                "                else:\n",
                "                    if buf.strip():\n",
                "                        chunks.append({'text': buf.strip(), 'source': str(pdf_path)})\n",
                "                    buf = seg\n",
                "        if buf.strip():\n",
                "            chunks.append({'text': buf.strip(), 'source': str(pdf_path)})\n",
                "\n",
                "        return chunks\n",
                "    except Exception as e:\n",
                "        print(f\"‚ö† Error processing {pdf_path}: {e}\")\n",
                "        return []\n",
                "\n",
                "def run_ner_redact(text):\n",
                "    if not ner_pipeline: return text\n",
                "    try:\n",
                "        ents = ner_pipeline(text)\n",
                "        for e in ents:\n",
                "            if e['entity_group'] == 'PER':\n",
                "                text = re.sub(r'\\b{}\\b'.format(re.escape(e['word'].replace(\"##\",\"\"))), '[REDACTED]', text)\n",
                "        return text\n",
                "    except:\n",
                "        return text"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- 5. GATHER & EMBED (WITH CACHING, ROBUST IDS & M1 TRACKING) ---\n",
                "import hashlib\n",
                "\n",
                "CACHE_DIR = Path(\"emb_cache_v2\")\n",
                "CACHE_DIR.mkdir(exist_ok=True, parents=True)\n",
                "\n",
                "# Global metric holder\n",
                "M1_AVG_EMBED_TIME = 0.0\n",
                "\n",
                "# --- 1. FILE FINDING ---\n",
                "print(\"--- PDF SELECTION ---\")\n",
                "\n",
                "# Mount Drive Logic\n",
                "if IN_COLAB:\n",
                "    from google.colab import drive\n",
                "    mount_choice = input(\"Mount Google Drive? (y/n): \").strip().lower()\n",
                "    if mount_choice == \"y\":\n",
                "        try:\n",
                "            drive.mount(\"/content/drive\", force_remount=True)\n",
                "            print(\"‚úÖ Drive mounted.\")\n",
                "        except Exception as e:\n",
                "            print(f\"‚ùå Mount failed: {e}\")\n",
                "            print(\"üí° TIP: Go to 'Runtime > Disconnect and Delete Runtime' to reset.\")\n",
                "\n",
                "folder_choice = input(\"Folder type (Civil/Criminal/Both) [both]: \").strip().lower() or \"both\"\n",
                "if folder_choice not in [\"civil\", \"criminal\", \"both\"]: folder_choice = \"both\"\n",
                "\n",
                "# Path Logic\n",
                "custom_path = input(\"Custom path? (Enter FILE PATH, not URL): \").strip()\n",
                "if custom_path.startswith(\"http\"):\n",
                "    print(\"‚ùå URL detected. Using search fallback.\")\n",
                "    custom_path = \"\"\n",
                "\n",
                "if custom_path:\n",
                "    search_dir = Path(custom_path)\n",
                "else:\n",
                "    search_dir = BASE_DIR\n",
                "    if IN_COLAB and not search_dir.exists(): search_dir = Path(\".\")\n",
                "\n",
                "print(f\"Searching in: {search_dir.absolute()}\")\n",
                "all_pdfs = list(search_dir.rglob(\"*.pdf\"))\n",
                "pdf_files = []\n",
                "\n",
                "if folder_choice == \"both\":\n",
                "    pdf_files = all_pdfs\n",
                "else:\n",
                "    term = folder_choice\n",
                "    pdf_files = [p for p in all_pdfs if term in str(p).lower()]\n",
                "\n",
                "print(f\"Found {len(pdf_files)} PDFs.\")\n",
                "if not pdf_files and IN_COLAB:\n",
                "    print(\"‚ùå No PDFs. Check your path or upload files.\")\n",
                "\n",
                "# --- 2. CACHING & EMBEDDING CORE ---\n",
                "def get_file_hash(file_path):\n",
                "    # Hash based on path + size + mod_time\n",
                "    try:\n",
                "        stat = file_path.stat()\n",
                "        sig = f\"{file_path.absolute()}_{stat.st_size}_{stat.st_mtime}\"\n",
                "        return hashlib.md5(sig.encode()).hexdigest()\n",
                "    except: return \"unknown\"\n",
                "\n",
                "def save_cache(cpath, vectors, chunks):\n",
                "    np.savez_compressed(cpath, vectors=vectors, chunks=chunks)\n",
                "\n",
                "def load_cache(cpath):\n",
                "    data = np.load(cpath, allow_pickle=True)\n",
                "    return data['vectors'], data['chunks']\n",
                "\n",
                "model_name = \"multi-qa-mpnet-base-cos-v1\"\n",
                "embedder = SentenceTransformer(model_name)\n",
                "if torch.cuda.is_available(): embedder.to(\"cuda\")\n",
                "\n",
                "def process_and_upload(pdf_files, batch_size=64):\n",
                "    print(f\"Processing {len(pdf_files)} files...\")\n",
                "    global M1_AVG_EMBED_TIME\n",
                "    \n",
                "    # Identify what needs embedding vs loading\n",
                "    to_embed = []\n",
                "    to_upload = [] # (vectors, chunks)\n",
                "    \n",
                "    for pdf in tqdm(pdf_files, desc=\"Checking Cache\"):\n",
                "        fhash = get_file_hash(pdf)\n        cpath = CACHE_DIR / f\"{fhash}.npz\"\n        \n        if cpath.exists():\n            try:\n                vecs, chks = load_cache(cpath)\n                to_upload.append((vecs, chks))\n            except:\n                to_embed.append((pdf, cpath))\n        else:\n            to_embed.append((pdf, cpath))\n            \n    # Process new files\n    if to_embed:\n        print(f\"‚ö° Embedding {len(to_embed)} new files...\")\n        t_start_all = time.time()\n        \n        for pdf, cpath in tqdm(to_embed, desc=\"Embedding New\"):\n            chunks = extract_chunks(str(pdf))\n            if not chunks: continue\n            \n            if USE_NER_REDACTION:\n                for c in chunks: c['text'] = run_ner_redact(c['text'])\n            \n            texts = [c['text'] for c in chunks]\n            \n            with torch.no_grad():\n                vectors = embedder.encode(texts, show_progress_bar=False, convert_to_numpy=True)\n                \n            save_cache(cpath, vectors, chunks)\n            to_upload.append((vectors, chunks))\n        \n        total_time = time.time() - t_start_all\n        # M1: Avg time per file (scaled to 10 files as per description usually means per-batch, but here we do per file avg)\n        if len(to_embed) > 0:\n            M1_AVG_EMBED_TIME = (total_time / len(to_embed)) * 10 \n            print(f\"üìä M1 (Time for 10 PDFs): {round(M1_AVG_EMBED_TIME, 2)}s\")\n            \n    # Upload to Pinecone\n    print(\"Syncing with Pinecone...\")\n    upsert_batch = []\n    \n    for vecs, chks in tqdm(to_upload, desc=\"Prep Upload\"):\n        for i, vec in enumerate(vecs):\n            chunk = chks[i]\n            # ISSUE 1 FIX: Deterministic SHA1 ID\n            unique_str = f\"{chunk['text']}_{chunk['source']}\"\n            uid = hashlib.sha1(unique_str.encode()).hexdigest()\n            \n            meta = {\n                'text': chunk['text'],\n                'source': chunk['source']\n            }\n            upsert_batch.append((uid, vec.tolist(), meta))\n            \n            if len(upsert_batch) >= batch_size:\n                index.upsert(vectors=upsert_batch)\n                upsert_batch = []\n                \n    if upsert_batch:\n        index.upsert(vectors=upsert_batch)\n        \n    print(\"‚úÖ Pipeline Complete.\")\n",
                "\n",
                "# Run\n",
                "if pdf_files:\n",
                "    process_and_upload(pdf_files)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- 6. RETRIEVAL & METRICS (FULL SUITE M1-M24) ---\n",
                "from sklearn.feature_extraction.text import CountVectorizer\n",
                "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
                "from nltk.translate.meteor_score import meteor_score\n",
                "\n",
                "# M2: Index Size\n",
                "M2_INDEX_SIZE = 0\n",
                "try:\n",
                "    stats = index.describe_index_stats()\n",
                "    M2_INDEX_SIZE = stats.get('total_vector_count', 0)\n",
                "except: pass\n",
                "\n",
                "class LegalEvaluator:\n",
                "    def __init__(self):\n",
                "        self.legal_terms = {\n",
                "            \"plaintiff\", \"defendant\", \"petitioner\", \"respondent\", \"appellant\", \n",
                "            \"writ\", \"jurisdiction\", \"affidavit\", \"statute\", \"provision\", \"act\",\n",
                "            \"section\", \"article\", \"constitution\", \"bench\", \"judgement\", \"decree\",\n",
                "            \"bail\", \"custody\", \"conviction\", \"acquittal\", \"prima facie\", \"locus standi\"\n",
                "        }\n",
                "        self.bias_terms = {\n",
                "            \"caste\", \"religion\", \"hindu\", \"muslim\", \"christian\", \"sikh\", \n",
                "            \"dalit\", \"brahmin\", \"shudra\", \"upper caste\", \"lower caste\",\n",
                "            \"gender\", \"female\", \"male\", \"race\", \"ethnicity\"\n",
                "        }\n",
                "        self.citation_pattern = re.compile(r\"(v\\.|vs\\.|versus|AIR \\d+|SCC \\d+|Section \\d+|Article \\d+)\", re.IGNORECASE)\n",
                "\n",
                "    def calculate_legal_scores(self, text):\n",
                "        words = set(re.findall(r\"\\w+\", text.lower()))\n",
                "        legal_matches = words.intersection(self.legal_terms)\n",
                "        term_precision = (len(legal_matches) / len(words) * 100) if words else 0.0\n",
                "\n",
                "        bias_matches = words.intersection(self.bias_terms)\n",
                "        bias_score = (len(bias_matches) / len(words) * 100) if words else 0.0\n",
                "\n",
                "        citations = self.citation_pattern.findall(text)\n",
                "        citation_count = len(citations)\n",
                "        cit_acc = 100.0 if citation_count > 0 else 0.0 # Simple proxy for now\n",
                "        \n",
                "        return term_precision, bias_score, cit_acc, citation_count\n",
                "\n",
                "def retrieve_answer(query, top_k=5):\n",
                "    t0 = time.time()\n",
                "    q_vec = embedder.encode([query])[0]\n",
                "    \n",
                "    try:\n",
                "        res = index.query(vector=q_vec.tolist(), top_k=top_k, include_metadata=True)\n",
                "        matches = res['matches']\n",
                "        texts = [m['metadata'].get('text', '') for m in matches]\n",
                "        scores = [m['score'] for m in matches]\n",
                "    except Exception as e:\n",
                "        print(f\"Retrieval Error: {e}\")\n",
                "        texts = []\n",
                "        scores = []\n",
                "        \n",
                "    context = \"\\n\\n\".join(texts)\n",
                "    t_retrieval = time.time() - t0\n",
                "\n",
                "    if not context:\n",
                "        return \"No context found.\", [], [], q_vec, t_retrieval, 0\n",
                "\n",
                "    t_gen_start = time.time()\n",
                "    prompt = f\"Answer using ONLY the context.\\n\\nContext:\\n{context}\\n\\nQuery: {query}\\nAnswer:\"\n",
                "    model = genai.GenerativeModel(\"gemini-1.5-flash\") \n",
                "    \n",
                "    try:\n",
                "        resp = model.generate_content(prompt)\n",
                "        answer = resp.text.strip()\n",
                "    except Exception as e:\n",
                "        answer = f\"[Gen Error: {e}]\"\n",
                "    \n",
                "    t_gen = time.time() - t_gen_start\n",
                "    return answer, texts, scores, q_vec, t_retrieval, t_gen\n",
                "\n",
                "def compute_detailed_metrics(preds, gts, ret_texts, ret_scores, lat_r, lat_g):\n",
                "    rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
                "    legal_eval = LegalEvaluator()\n",
                "    smoothie = SmoothingFunction().method4\n",
                "    \n",
                "    results = []\n",
                "    print(\"Computing metrics (M1-M24)...\")\n",
                "    \n",
                "    for i, (pred, gt, ret, scores) in enumerate(zip(preds, gts, ret_texts, ret_scores)):\n",
                "        # --- M1 & M2 ---\n",
                "        m1 = M1_AVG_EMBED_TIME if 'M1_AVG_EMBED_TIME' in globals() else 0.0\n",
                "        m2 = M2_INDEX_SIZE\n",
                "        \n",
                "        # --- M3 Retrieval Latency ---\n",
                "        m3 = lat_r[i]\n",
                "        \n",
                "        # --- M4 Cosine Similarity (Avg) ---\n",
                "        m4 = np.mean(scores) if scores else 0.0\n",
                "        \n",
                "        # --- M5 Top-k Accuracy ---\n",
                "        gt_words = set(gt.lower().split())\n",
                "        ctx_words = set(\" \".join(ret).lower().split())\n",
                "        m5 = 100.0 if len(gt_words.intersection(ctx_words)) > (len(gt_words) * 0.3) else 0.0\n",
                "        \n",
                "        # --- M6-M8 ROUGE ---\n",
                "        r_scores = rouge.score(gt, pred)\n",
                "        m6 = r_scores['rouge1'].fmeasure\n",
                "        m7 = r_scores['rouge2'].fmeasure\n",
                "        m8 = r_scores['rougeL'].fmeasure\n",
                "        \n",
                "        # --- M9 Context Length ---\n",
                "        m9 = len(\" \".join(ret).split())\n",
                "        \n",
                "        # --- M10-M12 Text Quality ---\n",
                "        m10 = sentence_bleu([gt.split()], pred.split(), smoothing_function=smoothie)\n",
                "        m11 = meteor_score([gt.split()], pred.split())\n",
                "        try:\n",
                "            P, R, F1 = bert_score([pred], [gt], lang=\"en\", rescale_with_baseline=True)\n",
                "            m12 = float(F1[0])\n",
                "        except: m12 = 0.0\n",
                "        \n",
                "        # --- M13 FCD ---\n",
                "        m13 = 0.0\n",
                "        if ret:\n",
                "            try:\n",
                "                _, _, F1_ctx = bert_score([pred], [\" \".join(ret)], lang=\"en\", rescale_with_baseline=True)\n",
                "                m13 = 100 * (1 - float(F1_ctx[0]))\n",
                "            except: pass\n",
                "            \n",
                "        # --- M14 Faithfulness ---\n",
                "        ans_words = set(pred.lower().split())\n",
                "        m14 = (len(ans_words.intersection(ctx_words)) / len(ans_words) * 100) if ans_words else 0.0\n",
                "        \n",
                "        # --- M15 GT Coverage ---\n",
                "        m15 = (len(gt_words.intersection(ans_words)) / len(gt_words) * 100) if gt_words else 0.0\n",
                "        \n",
                "        # --- M16-M19 System ---\n",
                "        m16 = lat_r[i] + lat_g[i]\n",
                "        m17 = 1 / m16 if m16 > 0 else 0\n",
                "        m18 = psutil.cpu_percent()\n",
                "        m19 = psutil.virtual_memory().used / (1024**3)\n",
                "        \n",
                "        # --- M20-M24 Legal ---\n",
                "        m21, m24, m20, cit_count = legal_eval.calculate_legal_scores(pred)\n",
                "        m22 = 100.0 if cit_count > 1 else 0.0\n",
                "        m23 = m13 # Duplicate\n",
                "        \n",
                "        results.append({\n",
                "            \"QID\": i,\n",
                "            \"M1_EmbedTime\": round(float(m1), 2), \"M2_IndexSize\": m2,\n",
                "            \"M3_RetLatency\": round(m3, 3), \"M4_CosSim\": round(m4, 3), \"M5_TopK_Acc\": m5,\n",
                "            \"M6_R1\": round(m6, 3), \"M7_R2\": round(m7, 3), \"M8_RL\": round(m8, 3),\n",
                "            \"M9_CtxLen\": m9, \"M10_BLEU\": round(m10, 3), \"M11_METEOR\": round(m11, 3),\n",
                "            \"M12_BERT_F1\": round(m12, 3), \"M13_FCD\": round(m13, 1),\n",
                "            \"M14_Faithful\": round(m14, 1), \"M15_GTCov\": round(m15, 1),\n",
                "            \"M16_E2ELat\": round(m16, 2), \"M17_Throughput\": round(m17, 2),\n",
                "            \"M18_CPU\": m18, \"M19_RAM\": round(m19, 2),\n",
                "            \"M20_CitAcc\": round(m20, 1), \"M21_TermPrec\": round(m21, 1),\n",
                "            \"M22_PrecCov\": m22, \"M23_FCD_dup\": round(m23, 1), \"M24_Bias\": round(m24, 1)\n",
                "        })\n",
                "        \n",
                "    return pd.DataFrame(results)\n",
                "\n",
                "# Run Evaluation\n",
                "queries = [\"Explain the 2001 SC/ST promotion case.\", \"Summarize a criminal case discussed in the documents.\"]\n",
                "gts = [\"The 2001 case regarding SC/ST reservation in promotion established that Article 16(4A) is an enabling provision...\", \"Criminal case summary placeholder...\"]\n",
                "\n",
                "preds, ctxs, sco, lr, lg = [], [], [], [], []\n",
                "\n",
                "print(\"\\n--- RUNNING EVALUATION ---\")\n",
                "for q in queries:\n",
                "    print(f\"Processing Q: {q}...\")\n",
                "    a, c, s, qv, tr, tg = retrieve_answer(q)\n",
                "    print(f\"A: {a[:150]}...\\n\")\n",
                "    preds.append(a)\n",
                "    ctxs.append(c)\n",
                "    sco.append(s)\n",
                "    lr.append(tr)\n",
                "    lg.append(tg)\n",
                "\n",
                "if preds:\n",
                "    df_metrics = compute_detailed_metrics(preds, gts, ctxs, sco, lr, lg)\n",
                "    from IPython.display import display\n",
                "    display(df_metrics)\n",
                "    df_metrics.to_csv(\"metrics_final.csv\", index=False)\n",
                "    print(\"‚úÖ Full M1-M24 Metrics saved to metrics_final.csv\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}