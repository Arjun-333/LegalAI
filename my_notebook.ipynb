{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55330234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello from Colab in VS Code!\n"
     ]
    }
   ],
   "source": [
    "# ===============================================\n",
    "# RAG PIPELINE v4 (OPTIMIZED: PARALLEL + ROBUST)\n",
    "# ===============================================\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import shutil\n",
    "import warnings\n",
    "import psutil\n",
    "import pickle\n",
    "import hashlib\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from transformers import logging as hf_logging\n",
    "hf_logging.set_verbosity_error()\n",
    "warnings.filterwarnings(\"ignore\", message=\"Some weights of the model\")\n",
    "\n",
    "# NLP / metrics libs\n",
    "import nltk\n",
    "for res in [\"punkt\", \"wordnet\", \"omw-1.4\"]:\n",
    "    try:\n",
    "        nltk.data.find(f\"tokenizers/{res}\")\n",
    "    except LookupError:\n",
    "        nltk.download(res, quiet=True)\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from bert_score import score as bert_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# =========================================================\n",
    "# ENVIRONMENT\n",
    "# =========================================================\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# -----------------------\n",
    "# OLLAMA AUTO-FIX\n",
    "# -----------------------\n",
    "# Try to find Ollama if not in PATH\n",
    "def find_ollama():\n",
    "    # 1. Check PATH\n",
    "    if shutil.which(\"ollama\"):\n",
    "        return \"ollama\"\n",
    "    \n",
    "    # 2. Check known Windows path\n",
    "    default_path = Path(os.environ[\"LOCALAPPDATA\"]) / \"Programs\" / \"Ollama\" / \"ollama.exe\"\n",
    "    if default_path.exists():\n",
    "        return str(default_path)\n",
    "    \n",
    "    return \"ollama\" # Fallback hope\n",
    "\n",
    "OLLAMA_CMD = find_ollama()\n",
    "print(f\"Ollama detected at: {OLLAMA_CMD}\")\n",
    "\n",
    "# -----------------------\n",
    "# GPU / device\n",
    "# -----------------------\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {DEVICE} | torch {torch.__version__}\")\n",
    "\n",
    "# -----------------------\n",
    "# PDF root\n",
    "# -----------------------\n",
    "BASE_DIR = Path(\"PDF\")\n",
    "\n",
    "# -----------------------\n",
    "# PDF Parsing (Helper for Parallel)\n",
    "# -----------------------\n",
    "try:\n",
    "    import fitz  # PyMuPDF\n",
    "    PDF_LIBRARY = \"pymupdf\"\n",
    "except Exception:\n",
    "    from pypdf import PdfReader\n",
    "    PDF_LIBRARY = \"pypdf\"\n",
    "\n",
    "def extract_chunks_from_file(pdf_path_str: str, chunk_size=200):\n",
    "    \"\"\"\n",
    "    Standalone function for multiprocessing. \n",
    "    Must take string paths, not Path objects, for maximum pickling compatibility.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pdf_path = Path(pdf_path_str)\n",
    "        if PDF_LIBRARY == \"pymupdf\":\n",
    "            doc = fitz.open(str(pdf_path))\n",
    "            text = \"\\n\".join([page.get_text(\"text\") for page in doc])\n",
    "            doc.close()\n",
    "        else:\n",
    "            from pypdf import PdfReader\n",
    "            reader = PdfReader(str(pdf_path))\n",
    "            text = \"\\n\".join([p.extract_text() or \"\" for p in reader.pages])\n",
    "\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "        sentences = re.split(r\"(?<=[.?!])\\s+\", text)\n",
    "\n",
    "        chunks = []\n",
    "        buf = \"\"\n",
    "        for sent in sentences:\n",
    "            if len((buf + \" \" + sent).split()) <= chunk_size:\n",
    "                buf = (buf + \" \" + sent).strip()\n",
    "            else:\n",
    "                if buf:\n",
    "                    chunks.append(buf.strip())\n",
    "                buf = sent.strip()\n",
    "        if buf:\n",
    "            chunks.append(buf.strip())\n",
    "        return chunks\n",
    "    except Exception as e:\n",
    "        # print(f\"âš  Error reading {pdf_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "# =========================================================\n",
    "# INCREMENTAL CACHING UTILS\n",
    "# =========================================================\n",
    "CACHE_DIR = Path(\"emb_cache_v2\")\n",
    "\n",
    "def get_file_hash(file_path: Path) -> str:\n",
    "    stat = file_path.stat()\n",
    "    sig = f\"{file_path.absolute()}_{stat.st_size}_{stat.st_mtime}\"\n",
    "    return hashlib.md5(sig.encode()).hexdigest()\n",
    "\n",
    "def save_file_cache(cache_path: Path, vectors: np.ndarray, texts: List[str]):\n",
    "    np.savez_compressed(cache_path, vectors=vectors, texts=texts)\n",
    "\n",
    "def load_file_cache(cache_path: Path):\n",
    "    data = np.load(cache_path, allow_pickle=True)\n",
    "    return data[\"vectors\"], data[\"texts\"]\n",
    "\n",
    "# =========================================================\n",
    "# CORE LOGIC: OPTIMIZED PROCESSING\n",
    "# =========================================================\n",
    "def process_and_embed_incrementally(pdf_files: List[Path], model_obj):\n",
    "    CACHE_DIR.mkdir(exist_ok=True)\n",
    "    \n",
    "    # 1. Identify which files need processing\n",
    "    files_to_process = []  # [(pdf, cache_path), ...]\n",
    "    cached_files = []      # [(pdf, cache_path), ...]\n",
    "    \n",
    "    print(f\"\\nScanning {len(pdf_files)} files for cache...\")\n",
    "    for pdf in pdf_files:\n",
    "        fhash = get_file_hash(pdf)\n",
    "        cpath = CACHE_DIR / f\"{fhash}.npz\"\n",
    "        if cpath.exists():\n",
    "            cached_files.append(cpath)\n",
    "        else:\n",
    "            files_to_process.append((pdf, cpath))\n",
    "\n",
    "    # 2. Extract & Embed (Auto-Switch based on OS)\n",
    "    new_chunks_map = {} # path -> chunks\n",
    "    \n",
    "    # Check OS: 'posix' = Linux/Mac (Parallel Safe), 'nt' = Windows (Serial Safer)\n",
    "    MAX_WORKERS = os.cpu_count() if os.name == 'posix' else 1\n",
    "    \n",
    "    if files_to_process:\n",
    "        if MAX_WORKERS > 1:\n",
    "            print(f\"\\nâš¡ Parallel Extracting {len(files_to_process)} NEW files using {MAX_WORKERS} cores (Linux/Mac)...\")\n",
    "            with ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "                future_to_file = {\n",
    "                    executor.submit(extract_chunks_from_file, str(pdf)): (pdf, cpath)\n",
    "                    for pdf, cpath in files_to_process\n",
    "                }\n",
    "                for future in tqdm(as_completed(future_to_file), total=len(files_to_process), desc=\"Extracting\"):\n",
    "                    pdf, cpath = future_to_file[future]\n",
    "                    try:\n",
    "                        chunks = future.result()\n",
    "                        if chunks:\n",
    "                            new_chunks_map[cpath] = chunks\n",
    "                    except Exception as e:\n",
    "                        print(f\"Parallel Error {pdf}: {e}\")\n",
    "        else:\n",
    "            print(f\"\\nðŸ¢ Serial Extracting {len(files_to_process)} NEW files (Windows Safe Mode)...\")\n",
    "            for pdf, cpath in tqdm(files_to_process, desc=\"Extracting\"):\n",
    "                chunks = extract_chunks_from_file(str(pdf))\n",
    "                if chunks:\n",
    "                    new_chunks_map[cpath] = chunks\n",
    "\n",
    "    # 3. Embedding loop (chunks -> vectors)\n",
    "    # We process in batches and SAVE FREQUENTLY (Crash Recovery)\n",
    "    if new_chunks_map:\n",
    "        print(f\"\\nðŸ”¹ Embedding {len(new_chunks_map)} files (saving every step)...\")\n",
    "        \n",
    "        keys = list(new_chunks_map.keys())\n",
    "        \n",
    "        for cpath in tqdm(keys, desc=\"Embedding & Saving\"):\n",
    "            chunks = new_chunks_map[cpath]\n",
    "            if not chunks: continue\n",
    "                \n",
    "            # Embed this file's chunks\n",
    "            vecs = model_obj.encode(chunks, batch_size=64, show_progress_bar=False, convert_to_numpy=True)\n",
    "            \n",
    "            # Save IMMEDIATELY\n",
    "            save_file_cache(cpath, vecs, chunks)\n",
    "            cached_files.append(cpath) # Mark as done\n",
    "\n",
    "    # 4. Load everything from cache\n",
    "\n",
    "    # 4. Load everything from cache\n",
    "    print(\"\\nðŸ“¦ Loading all data from cache...\")\n",
    "    all_vectors = []\n",
    "    all_texts = []\n",
    "    \n",
    "    for cpath in tqdm(cached_files, desc=\"Loading Cache\"):\n",
    "        try:\n",
    "            v, t = load_file_cache(cpath)\n",
    "            if len(v) > 0:\n",
    "                all_vectors.append(v)\n",
    "                all_texts.extend(t)\n",
    "        except Exception as e:\n",
    "            print(f\"Corrupt cache {cpath.name}, skipping.\")\n",
    "\n",
    "    if not all_vectors:\n",
    "        return np.array([]), []\n",
    "\n",
    "    final_matrix = np.vstack(all_vectors)\n",
    "    return final_matrix, all_texts\n",
    "\n",
    "def gather_pdfs(base: Path, folder=\"CIVIL\", years=None, all_flag=True):\n",
    "    folder_path = base / folder\n",
    "    if not folder_path.exists(): return []\n",
    "    pdfs = []\n",
    "    if all_flag or years is None:\n",
    "        pdfs = list(folder_path.rglob(\"*.pdf\"))\n",
    "    else:\n",
    "        for y in years:\n",
    "            p = folder_path / str(y)\n",
    "            if p.exists(): pdfs.extend(p.rglob(\"*.pdf\"))\n",
    "    return pdfs\n",
    "\n",
    "# =========================================================\n",
    "# LLM & RETRIEVAL\n",
    "# =========================================================\n",
    "def generate_answer_llm(prompt: str, llm_backend: str) -> str:\n",
    "    prompt = prompt[:16000]\n",
    "    \n",
    "   if llm_backend == \"gemini\":\n",
    "    GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "    if not GOOGLE_API_KEY:\n",
    "        return \"[Error: GOOGLE_API_KEY missing]\"\n",
    "    try:\n",
    "        import google.generativeai as genai\n",
    "        genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "        # Gemini 1.5 Pro \n",
    "        model = genai.GenerativeModel(\n",
    "            model_name=\"gemini-1.5-pro\",\n",
    "            generation_config={\n",
    "                \"temperature\": 0.1,        \n",
    "                \"top_p\": 0.8,\n",
    "                \"top_k\": 40, \n",
    "                \"max_output_tokens\": 1024  \n",
    "            }\n",
    "        )\n",
    "\n",
    "        response = model.generate_content(\n",
    "            prompt,\n",
    "            safety_settings={\n",
    "                \"HARASSMENT\": \"block_none\",\n",
    "                \"HATE\": \"block_none\",\n",
    "                \"SEXUAL\": \"block_none\",\n",
    "                \"DANGEROUS\": \"block_none\"\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return response.text.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"[Gemini Error: {e}]\"\n",
    "\n",
    "\n",
    "    if llm_backend in [\"gpt4\", \"gpt35\"]:\n",
    "        try:\n",
    "            from openai import OpenAI\n",
    "            client = OpenAI()\n",
    "            model = \"gpt-4o\" if llm_backend == \"gpt4\" else \"gpt-3.5-turbo\"\n",
    "            resp = client.chat.completions.create(model=model, messages=[{\"role\": \"user\", \"content\": prompt}], temperature=0)\n",
    "            return resp.choices[0].message.content.strip()\n",
    "        except Exception as e: return f\"[OpenAI Error: {e}]\"\n",
    "\n",
    "    if llm_backend == \"claude\":\n",
    "        try:\n",
    "            import anthropic\n",
    "            client = anthropic.Anthropic()\n",
    "            msg = client.messages.create(model=\"claude-3-haiku-20240307\", max_tokens=512, temperature=0, messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "            return msg.content[0].text.strip()\n",
    "        except Exception as e: return f\"[Claude Error: {e}]\"\n",
    "\n",
    "    if llm_backend in [\"llama3\", \"mistral\", \"phi3\"]:\n",
    "        import requests\n",
    "        model_map = {\"llama3\": \"llama3:8b\", \"mistral\": \"mistral:7b\", \"phi3\": \"phi3:mini\"}\n",
    "        # Use our auto-detected OLLAMA if needed for CLI, but here it's HTTP API (localhost:11434)\n",
    "        # Assuming Ollama app is running in background.\n",
    "        try:\n",
    "            r = requests.post(\"http://localhost:11434/api/generate\", json={\"model\": model_map.get(llm_backend, \"llama3\"), \"prompt\": prompt, \"stream\": False}, timeout=120)\n",
    "            r.raise_for_status()\n",
    "            return r.json()[\"response\"].strip()\n",
    "        except Exception as e: return f\"[Ollama Error: {e}]\"\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "def retrieve_answer(query, model_obj, vectors, texts_list, llm_backend, top_k=5):\n",
    "    q_vec = model_obj.encode([query])[0]\n",
    "    sims = cosine_similarity([q_vec], vectors)[0]\n",
    "    top_ids = np.argsort(sims)[-top_k:][::-1]\n",
    "    retrieved = [texts_list[i] for i in top_ids]\n",
    "    context = \"\\n\\n\".join(retrieved)\n",
    "    \n",
    "    if llm_backend and llm_backend != \"none\":\n",
    "        prompt = f\"Use ONLY the context below to answer.\\n\\nContext:\\n{context}\\n\\nQ: {query}\\nA:\"\n",
    "        answer = generate_answer_llm(prompt, llm_backend)\n",
    "    else:\n",
    "        answer = retrieved[0] if retrieved else \"\"\n",
    "\n",
    "    return answer, retrieved, np.array(q_vec, dtype=float)\n",
    "\n",
    "# =========================================================\n",
    "# METRICS HELPER (M1-M24)\n",
    "# =========================================================\n",
    "class LegalEvaluator:\n",
    "    def __init__(self):\n",
    "        # M21: Terminology Precision\n",
    "        self.legal_terms = {\n",
    "            \"plaintiff\", \"defendant\", \"petitioner\", \"respondent\", \"appellant\", \n",
    "            \"writ\", \"jurisdiction\", \"affidavit\", \"statute\", \"provision\", \"act\",\n",
    "            \"section\", \"article\", \"constitution\", \"bench\", \"judgement\", \"decree\",\n",
    "            \"bail\", \"custody\", \"conviction\", \"acquittal\", \"prima facie\", \"locus standi\"\n",
    "        }\n",
    "        # M24: Bias Score (Protected attributes)\n",
    "        self.bias_terms = {\n",
    "            \"caste\", \"religion\", \"hindu\", \"muslim\", \"christian\", \"sikh\", \n",
    "            \"dalit\", \"brahmin\", \"shudra\", \"upper caste\", \"lower caste\",\n",
    "            \"gender\", \"female\", \"male\", \"race\", \"ethnicity\"\n",
    "        }\n",
    "        # M20: Citations (Basic Regex)\n",
    "        self.citation_pattern = re.compile(r\"(v\\.|vs\\.|versus|AIR \\d+|SCC \\d+|Section \\d+|Article \\d+)\", re.IGNORECASE)\n",
    "\n",
    "    def calculate_legal_scores(self, text):\n",
    "        words = set(re.findall(r\"\\w+\", text.lower()))\n",
    "        \n",
    "        # M21: Usage of legal terms\n",
    "        legal_matches = words.intersection(self.legal_terms)\n",
    "        term_precision = len(legal_matches) / len(words) if words else 0.0\n",
    "\n",
    "        # M24: Bias presence\n",
    "        bias_matches = words.intersection(self.bias_terms)\n",
    "        bias_score = (len(bias_matches) / len(words)) * 100 if words else 0.0\n",
    "\n",
    "        # M20: Citations found\n",
    "        citations = self.citation_pattern.findall(text)\n",
    "        citation_count = len(citations)\n",
    "        \n",
    "        return term_precision, bias_score, citation_count\n",
    "\n",
    "def evaluate_advanced(preds, gts, retrieved_list, q_vecs, embedder_obj, latencies_r, latencies_g, vectors, texts_list, log_file=\"metrics_log.csv\"):\n",
    "    df_list = []\n",
    "    \n",
    "    # Init Scorers\n",
    "    rouge = rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeL\"], use_stemmer=True)\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    legal_eval = LegalEvaluator()\n",
    "    \n",
    "    # M2: Index Size\n",
    "    index_size_vectors = len(vectors) \n",
    "\n",
    "    for i, (pred, gt, ret, qv) in enumerate(zip(preds, gts, retrieved_list, q_vecs)):\n",
    "        # --- Answer Quality (M6-M15, M23) ---\n",
    "        r_scores = rouge.score(gt, pred)\n",
    "        # M6-M8\n",
    "        r1_f = r_scores[\"rouge1\"].fmeasure\n",
    "        r2_f = r_scores[\"rouge2\"].fmeasure\n",
    "        rl_f = r_scores[\"rougeL\"].fmeasure\n",
    "        \n",
    "        # M10: BLEU\n",
    "        bleu = sentence_bleu([gt.split()], pred.split(), smoothing_function=smoothie)\n",
    "        \n",
    "        # M11: METEOR\n",
    "        meteor = meteor_score([gt.split()], pred.split())\n",
    "        \n",
    "        # M12: BERTScore\n",
    "        try:\n",
    "            P, R, F1 = bert_score([pred], [gt], lang=\"en\", rescale_with_baseline=True)\n",
    "            bert_f = float(F1[0])\n",
    "        except Exception: bert_f = 0.0\n",
    "        \n",
    "        # M13/M23: Factual Consistency Deviation\n",
    "        fcd = (1 - bert_f) * 100 if bert_f > 0 else 100.0\n",
    "\n",
    "        # M9: Context Length (Tokens approx by words for speed or simple split)\n",
    "        ctx_len = sum(len(c.split()) for c in ret)\n",
    "\n",
    "        # M14: Faithfulness (Simple Overlap approx)\n",
    "        # % of answer words that appear in context\n",
    "        ans_words = set(pred.lower().split())\n",
    "        ctx_words = set(\" \".join(ret).lower().split())\n",
    "        faithfulness = len(ans_words.intersection(ctx_words)) / len(ans_words) if ans_words else 0.0\n",
    "\n",
    "        # M15: GT Coverage\n",
    "        gt_words = set(gt.lower().split())\n",
    "        gt_cov = len(ans_words.intersection(gt_words)) / len(gt_words) if gt_words else 0.0\n",
    "\n",
    "        # --- Retrieval Performance (M1-M5) ---\n",
    "        # M3: Retrieval Latency -> latencies_r[i]\n",
    "        # M4: Cosine Similarity\n",
    "        if len(ret) > 0:\n",
    "            ret_vec = embedder_obj.encode([ret[0]])[0] \n",
    "            cosine_sim = float(cosine_similarity([qv], [ret_vec])[0][0])\n",
    "        else: cosine_sim = 0.0\n",
    "        \n",
    "        # M5: Top-k Accuracy (Approx: is GT substantially present in Context?)\n",
    "        # Simple check: do 30% of GT words appear in Context?\n",
    "        is_in_top_k = 1 if (len(gt_words.intersection(ctx_words)) / len(gt_words) > 0.3 if gt_words else 0) else 0\n",
    "\n",
    "        # --- System Efficiency (M16-M19) ---\n",
    "        # M16: End-to-End Latency\n",
    "        e2e_lat = latencies_r[i] + latencies_g[i]\n",
    "        # M17: Throughput (queries/sec for this single query)\n",
    "        throughput = 1 / e2e_lat if e2e_lat > 0 else 0\n",
    "        # M18/M19: System\n",
    "        cpu_use = psutil.cpu_percent()\n",
    "        ram_use_gb = psutil.virtual_memory().used / (1024**3)\n",
    "\n",
    "        # --- Legal Specific (M20-M22, M24) ---\n",
    "        term_prec, bias_score, cit_count = legal_eval.calculate_legal_scores(pred)\n",
    "        # M20: Citation Accuracy (proxy: did we find citations?)\n",
    "        cit_acc = 100.0 if cit_count > 0 else 0.0 \n",
    "        \n",
    "        # Assemble Row\n",
    "        df_list.append({\n",
    "            \"QID\": i,\n",
    "            \"M1_EmbedTime\": \"Cached\", # Constant\n",
    "            \"M2_IndexSize\": index_size_vectors,\n",
    "            \"M3_RetLatency\": round(latencies_r[i], 3),\n",
    "            \"M4_CosSim\": round(cosine_sim, 3),\n",
    "            \"M5_TopK_Acc\": is_in_top_k,\n",
    "            \"M6_R1\": round(r1_f, 3),\n",
    "            \"M7_R2\": round(r2_f, 3),\n",
    "            \"M8_RL\": round(rl_f, 3),\n",
    "            \"M9_CtxLen\": ctx_len,\n",
    "            \"M10_BLEU\": round(bleu, 3),\n",
    "            \"M11_METEOR\": round(meteor, 3),\n",
    "            \"M12_BERT_F1\": round(bert_f, 3),\n",
    "            \"M13_FCD\": round(fcd, 1),\n",
    "            \"M14_Faithfulness\": round(faithfulness * 100, 1),\n",
    "            \"M15_GTCov\": round(gt_cov * 100, 1),\n",
    "            \"M16_E2E_Lat\": round(e2e_lat, 2),\n",
    "            \"M17_Throughput\": round(throughput, 2),\n",
    "            \"M18_CPU\": cpu_use,\n",
    "            \"M19_RAM\": round(ram_use_gb, 2),\n",
    "            \"M20_CitAcc\": cit_acc,\n",
    "            \"M21_TermPrec\": round(term_prec * 100, 1),\n",
    "            \"M22_PrecCov\": 100 if len(ret) > 1 else 0, # Did we get >1 doc?\n",
    "            \"M23_FCD_dup\": round(fcd, 1),\n",
    "            \"M24_Bias\": round(bias_score, 1)\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(df_list)\n",
    "    df.to_csv(log_file, index=False)\n",
    "    print(f\"\\nâœ… Full M1-M24 Metrics saved to {log_file}\")\n",
    "    return df\n",
    "\n",
    "# =========================================================\n",
    "# MAIN\n",
    "# =========================================================\n",
    "def main():\n",
    "    print(\"\\n--- RAG PIPELINE v4 (OPTIMIZED) ---\")\n",
    "    \n",
    "    # Selection\n",
    "    folder_choice = input(\"Which PDF type? (civil/criminal/both) [both]: \").strip().lower() or \"both\"\n",
    "    process_all = input(\"Process ALL PDFs? (y/n) [y]: \").strip().lower() or \"y\"\n",
    "    \n",
    "    years_list = None\n",
    "    if process_all != \"y\":\n",
    "        year_input = input(\"Enter years (e.g., 2000, 2002-2005): \").strip()\n",
    "        years_list = []\n",
    "        for token in year_input.split(\",\"):\n",
    "            if \"-\" in token:\n",
    "                a, b = map(int, token.split(\"-\"))\n",
    "                years_list.extend(range(a, b + 1))\n",
    "            else:\n",
    "                years_list.append(int(token))\n",
    "\n",
    "    pdf_files = []\n",
    "    if folder_choice in [\"civil\", \"both\"]:\n",
    "        pdf_files += gather_pdfs(BASE_DIR, \"CIVIL\", years_list, process_all == \"y\")\n",
    "    if folder_choice in [\"criminal\", \"both\"]:\n",
    "        pdf_files += gather_pdfs(BASE_DIR, \"CRIMINAL\", years_list, process_all == \"y\")\n",
    "\n",
    "    print(f\"\\nTotal PDFs found: {len(pdf_files)}\")\n",
    "    if not pdf_files: return\n",
    "\n",
    "    # Model\n",
    "    model_name = \"multi-qa-mpnet-base-cos-v1\"\n",
    "    print(f\"\\nLoading Embedding Model: {model_name}...\")\n",
    "    model_obj = SentenceTransformer(model_name)\n",
    "\n",
    "    # PROCESS\n",
    "    t0 = time.time()\n",
    "    vectors, texts = process_and_embed_incrementally(pdf_files, model_obj)\n",
    "    t1 = time.time()\n",
    "    print(f\"\\nCompleted in {t1-t0:.2f}s | Vectors: {vectors.shape}\")\n",
    "\n",
    "    if len(vectors) == 0: return\n",
    "\n",
    "    # LLM Loop\n",
    "    while True:\n",
    "        print(\"\\n--- LLM SELECTION ---\")\n",
    "        print(\"1) GPT-4 (gpt4)\")\n",
    "        print(\"2) GPT-3.5 (gpt35)\")\n",
    "        print(\"3) Claude 3 Haiku (claude)\")\n",
    "        print(\"4) LLaMA 3 (llama3)\")\n",
    "        print(\"5) Mistral (mistral)\")\n",
    "        print(\"6) Phi-3 (phi3)\")\n",
    "        print(\"7) Gemini (gemini)\")\n",
    "        print(\"8) None (Retrieval Only)\")\n",
    "        print(\"q) Quit\")\n",
    "        \n",
    "        llm_input = input(\"Choice: \").strip().lower()\n",
    "        if llm_input in [\"q\", \"quit\"]: break\n",
    "        \n",
    "        llm_map = {\n",
    "            \"1\": \"gpt4\", \"2\": \"gpt35\", \"3\": \"claude\", \n",
    "            \"4\": \"llama3\", \"5\": \"mistral\", \"6\": \"phi3\", \"7\": \"gemini\", \"8\": \"none\"\n",
    "        }\n",
    "        llm_backend = llm_map.get(llm_input, llm_input) \n",
    "\n",
    "        print(f\"\\nRunning tests with LLM={llm_backend}\")\n",
    "        queries = [\"Explain the 2001 SC/ST promotion case.\", \"Summarize a criminal case discussed in the documents.\"]\n",
    "        gts = [\"SC/ST promotion case details...\", \"Criminal case summary...\"]\n",
    "        \n",
    "        preds, ret, q_vecs, lat_r, lat_g = [], [], [], [], []\n",
    "\n",
    "        for q in queries:\n",
    "            t0 = time.time()\n",
    "            ans, r, qv = retrieve_answer(q, model_obj, vectors, texts, llm_backend)\n",
    "            t1 = time.time()\n",
    "            print(f\"\\nQ: {q}\\nA: {ans[:200]}...\")\n",
    "            preds.append(ans)\n",
    "            ret.append(r)\n",
    "            q_vecs.append(qv)\n",
    "            lat_r.append(0.5)\n",
    "            lat_g.append(t1 - t0 - 0.5)\n",
    "\n",
    "        evaluate_advanced(preds, gts, ret, q_vecs, model_obj, lat_r, lat_g, vectors, texts)\n",
    "        \n",
    "        input(\"\\nPress Enter to continue loop...\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Windows Mulitprocessing support\n",
    "    import multiprocessing\n",
    "    multiprocessing.freeze_support()\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
